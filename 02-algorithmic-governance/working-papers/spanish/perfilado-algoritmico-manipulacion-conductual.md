**Violaciones de Protección de Datos y Privacidad**

**a través del Perfilado Algoritmico y Manipulación Conductual:**

**los retos frente a la regulación Europea y Española.**

En la era digital contemporánea, la privacidad no se vulnera únicamente a través de la recopilación masiva de datos personales. Existe una dimensión más insidiosa y menos visible de la violación de derechos fundamentales: aquella que ocurre a través de la manipulación deliberada de las decisiones de los usuarios mediante el diseño de interfaces que, aparentemente funcionales y neutras, están específicamente diseñadas para influenciar comportamientos en formas potencialmente perjudiciales. Los denominados "patrones oscuros" (dark patterns) representan una categoría emergente de violaciones de derechos que la legislación europea está apenas comenzando a abordar de forma sistemática

Según la Agencia Española de Protección de Datos (AEPD), los dark patterns son "interfaces e implementaciones de experiencia de usuario destinadas a influenciar en el comportamiento y las decisiones de las personas en su interacción con webs, apps y redes sociales, de forma que tomen decisiones potencialmente perjudiciales para la protección de sus datos personales"[\[1\]](#footnote-2). La magnitud del problema es alarmante: estudios académicos interdisciplinarios recientes revelan que estas prácticas son sistémicas, no excepcionales, penetrando la mayoría de los servicios digitales más populares[\[2\]](#footnote-3).

Al respecto, resulta menester preguntarse: ¿Cómo puede existir una verdadera privacidad y protección de datos si los usuarios son constantemente manipulados a través de interfaces diseñadas deliberadamente para debilitar su autonomía decisoria? Y aledañamente: ¿Cuales son los contrapesos establecidos por la legislación Europea y Española para asegurar la protección de los derechos de las personas usuarias ante esta manipulación? Estas preguntas motivan el presente ensayo.

La vulneración de privacidad a través de perfilado algorítmico y manipulación conductual existe en la intersección de tres marcos regulatorios europeos. El RGPD (artículos 5.1.a y 22) establece el principio de "licitud, lealtad y transparencia" en el tratamiento de datos personales[\[3\]](#footnote-4) y reconoce un derecho a no ser objeto de decisiones automatizadas que produzcan efectos jurídicos significativos[\[4\]](#footnote-5). Sin embargo, este marco se enfoca tradicionalmente en protección de datos más que en protección contra manipulación conductual. La Ley de Inteligencia Artificial (artículo 5.1.a y 5.1.b) prohíbe explícitamente sistemas que utilicen técnicas manipuladoras[\[5\]](#footnote-6), reconociendo así que el RGPD por sí solo no es suficiente para proteger contra estas prácticas emergentes. Finalmente, la AEPD ha comenzado a sancionar específicamente el uso de dark patterns como violaciones del principio de lealtad del RGPD, estableciendo jurisprudencia administrativa sobre esta materia que se abordará en el próximo apartado.

**Los Dark Patterns, Patrones adictivos y manipulación digital**

La vulneración de protección de datos a través del perfilado algorítmico y la manipulación conductual no es un fenómeno excepcional. Se trata de una categoría específica y sistemática de violaciones que operan en la intersección entre interfaces de usuario diseñadas deliberadamente para manipular decisiones y sistemas que explotan vulnerabilidades psicológicas humanas[\[6\]](#footnote-7)

Como se comentó ad supra, los dark patterns son interfaces destinadas a influenciar y afectar negativamente el comportamiento y decisiones de las personas usuarias. Jurídicamente, estos patrones violan el artículo 5.1.a) del RGPD que establece que los datos personales serán tratados de manera "lícita, leal y transparente en relación con el interesado"[\[7\]](#footnote-8). En este sentido, el principio de lealtad no es una simple obligación de información genérica; es una obligación sustantiva de no manipular deliberadamente a los interesados en sus decisiones sobre datos. La AEPD ha especificado explícitamente que "en aplicación del principio de lealtad establecido en el artículo 5.1.a, los responsables del tratamiento han de garantizar que no se emplean patrones oscuros, al menos, con relación a las decisiones respecto del tratamiento de sus datos personales" [\[8\]](#footnote-9)

El Comité Europeo de Protección de Datos (EDPB) ha desarrollado una clasificación de dark patterns que la AEPD ha adoptado. Los cinco tipos principales son: Sobrecarga[\[9\]](#footnote-10), que genera fatiga cognitiva mediante múltiples opciones simultáneas; Ocultación[\[10\]](#footnote-11), que oculta información importante; Emoción[\[11\]](#footnote-12), que apela a emociones para influenciar decisiones; Obstaculización[\[12\]](#footnote-13), que crea dificultades técnicas para ejercer derechos; e Inconsistencia[\[13\]](#footnote-14), que presenta diseño inestable donde controles cambian sin aviso.

Mientras que los dark patterns atacan la libertad de decisión a través del diseño de interfaces de usuario, existe una categoría más amplia de prácticas manipuladoras: los patrones adictivos. La AEPD los define como "una característica, atributo o práctica de diseño que determina una manera concreta de utilizar plataformas, aplicaciones o servicios digitales que persigue que los usuarios dediquen mucho más tiempo a utilizarlos o con un mayor grado de compromiso del esperado, conveniente o saludable para ellos"[\[14\]](#footnote-15). Los patrones adictivos funcionan explotando sesgos cognitivos y vulnerabilidades psicológicas específicas del usuario. La AEPD clasifica estos patrones en cuatro categorías principales: Acción Forzada[\[15\]](#footnote-16), que ofrece algo deseable a cambio de exigencias o engaños (autoplay, scroll infinito, notificaciones push); Ingeniería Social[\[16\]](#footnote-17), que manipula mediante sesgos cognitivos (escasez artificial, prueba social, contadores regresivos falsos); Interferencia en la Interfaz[\[17\]](#footnote-18), que manipula elementos visuales para promover acciones; y Persistencia[\[18\]](#footnote-19), que explota efectos psicológicos como el efecto Zeigarnik.

Lo significativo es que la AEPD ha reconocido que cuando estos patrones adictivos están relacionados con "técnicas subliminales, manipulativas, engañosas o que exploten vulnerabilidades de personas físicas o grupos de personas", especialmente por edad[\[19\]](#footnote-20), por lo que, de estar mediados por sistemas automatizados de IA, estaríamos ante el supuesto de comportamientos prohibidos en los términos del artículo 5 del Reglamento Europeo de Inteligencia Artificial. Este reconocimiento apunta hacia la categoría más grave de manipulación: las técnicas subliminales prohibidas por la Ley de IA. En este sentido, el artículo 5.1.a) de la Ley de IA establece que quedan prohibidas "la comercialización, la puesta en servicio o el uso de un sistema de IA que utilice técnicas subliminales que escapen a la conciencia de una persona o técnicas deliberadamente manipuladoras o engañosas, con el objetivo o el efecto de distorsionar materialmente el comportamiento de una persona"[\[20\]](#footnote-21). Esta es una prohibición absoluta, no simplemente una norma regulatoria con sanciones flexibles, es decir, no hay conformidad parcial: el sistema no puede ser utilizado bajo ninguna circunstancia a tenor del artículo 5.1.a).

Los dark patterns, patrones adictivos y técnicas subliminales constituyen violaciones que operan en múltiples niveles del ordenamiento jurídico europeo. A nivel del RGPD, violan el principio de lealtad como obligación sustantiva de no manipular deliberadamente. A nivel de la Ley de IA, constituyen sistemas expresamente prohibidos. Y a nivel práctico, la AEPD ya ha comenzado a sancionar estas prácticas como infracciones concretas del RGPD[\[21\]](#footnote-22). El perfilado algorítmico combinado con estos mecanismos de manipulación no representa un problema marginal sino una categoría específica de violación fundamental de derechos que afecta autonomía, privacidad y dignidad.

**El problema con el perfilado algorítmico y el consentimiento informado (bajo presión)**

Existe una paradoja central del consentimiento en el contexto de perfilado algorítmico: mientras el RGPD exige que sea **"libre, específico, informado e inequívoco"**, en la práctica el consentimiento a perfilado se obtiene bajo presión de acceso a servicios (los usuarios no podemos utilizar plataformas digitales sin consentir el perfilado) y mediante dark patterns (interfaces diseñadas para manipular decisiones), según se comentó anteriormente. Esta combinación convierte el consentimiento prácticamente en una ficción legal: válido en el papel, pero sustancialmente nulo en la práctica.

El RGPD define consentimiento en su artículo 4.11 como **"toda manifestación de voluntad libre, específica, informada e inequívoca por la que el interesado acepta, ya sea mediante una declaración o una clara acción afirmativa, el tratamiento de datos personales que le conciernen"[\[22\]](#footnote-23)**. En este sentido, y según sus elementos esenciales, el consentimiento debe ser: **Libre, en el sentido que** la persona no debe haber sido presionada para dar su consentimiento ni sufrir perjuicio si lo rechaza; **Específico, implicando que** se le debe pedir a la persona que consienta a tipos individuales específicos de tratamiento de datos; **Informado, en donde** se debe informar a la persona a qué está consintiendo; **Inequívoco, pues** el lenguaje debe ser claro y sencillo; y debe constituir una clara **Acción afirmativa, es decir, que** la persona debe consentir expresamente haciendo o diciendo algo, no por silencio o inactividad[\[23\]](#footnote-24).

El Comité Europeo de Protección de Datos (EDPB) ha establecido en sus **Directrices 5/2020 sobre Consentimiento** que **"el consentimiento debe ser libre, lo que implica elección y control reales por parte de los interesados[\[24\]](#footnote-25). Se entiende que si el sujeto no es realmente libre para elegir, se siente obligado a dar su consentimiento o sufrirá consecuencias negativas si no lo da, entonces el consentimiento no puede considerarse válido.** El EDPB especifica explícitamente que **"si el consentimiento está incluido como una parte no negociable de las condiciones generales se asume que no se ha dado libremente[\[25\]](#footnote-26). En consecuencia, se puede afirmar que no se puede considerar que el consentimiento se ha prestado libremente si el interesado no puede negar o retirar su consentimiento sin perjuicio.**

Esta directriz es crítica en el contexto de perfilado algorítmico. Los usuarios de redes sociales, motores de búsqueda, y plataformas de streaming no podemos acceder a estos servicios sin consentir a un perfilado exhaustivo para publicidad comportamental[\[26\]](#footnote-27). No existe opción de usar YouTube sin aceptar que Google recopile nuestros datos sobre preferencias para personalización de recomendaciones[\[27\]](#footnote-28). No existe opción de usar Instagram sin aceptar que Meta construya un perfil psicográfico para publicidad dirigida[\[28\]](#footnote-29). En este sentido, el consentimiento es **una condición no negociable de acceso**, por lo que no una opción genuina para con las personas usuarias.

****La magnitud del problema jurídico: dark patterns + consentimiento = nulidad****

Cuando se combinan dark patterns con presión de acceso, la nulidad del consentimiento se amplifica. El EDPB proporciona un ejemplo específicamente relevante para plataformas digitales: **"Un proveedor de sitios web introduce un script que oculta el contenido, a excepción de una solicitud de aceptar las cookies e información sobre las cookies utilizadas y los fines para los que se tratarán los datos. No es posible acceder al contenido sin pulsar el botón 'aceptar las cookies'. Dado que al interesado no se le ofrece una posibilidad real de elección, su consentimiento no se manifiesta libremente"[\[29\]](#footnote-30)**. El EDPB concluye: **"Esto no implica un consentimiento válido, ya que la prestación del servicio se supedita a que el interesado pulse el botón 'aceptar las cookies'. No se le ofrece una posibilidad real de elección"[\[30\]](#footnote-31)**

Este análisis se extiende directamente al perfilado algorítmico en redes sociales. Un usuario de TikTok que desea usar la plataforma enfrenta una interfaz donde rechazar el perfilado requiere múltiples clics, navegación a menús profundos, o simplemente es imposible, pues el consentimiento solo es la opción funcional (dark pattern de **obstaculización**)[\[31\]](#footnote-32). La presión de acceso (no puedo usar TikTok sin consentir perfilado) combinada con dark patterns (rechazar es prácticamente imposible) genera una **presunción legal de que el consentimiento no se ha dado libremente**.

****Opacidad algorítmica como barrera a consentimiento "informado"****

Más allá de los problemas de "libre" y "específico", existe un tercer problema: el consentimiento **no puede ser "informado"** cuando la lógica del algoritmo es opaca [\[32\]](#footnote-33).

Para que el consentimiento sea "informado", a tenor del artículo 7.3 del RPGD, el usuario debe entender: (1) **qué datos se recopilan (**nombre, ubicación, historial de búsqueda, patrón de interacción); (2) **cómo se utilizarán (**qué modelo de machine learning los procesará, qué características extraerá, qué predicciones hará); (3) **qué decisiones resultarán (**qué contenido se recomienda, qué se oculta, qué anuncios se muestran); y (4) **quiénes tendrán acceso (**anunciantes, data brokers, gobiernos)[\[33\]](#footnote-34)

Preocupantemente, las plataformas frecuentemente invocan **secretos comerciales e inteligencia artificial propietaria** para negar información específica sobre cómo funcionan sus sistemas de recomendación. Una persona que lee la política de privacidad de una red social moderna encontrará afirmaciones genéricas como "utilizamos machine learning para personalizar contenido", pero **nunca sabrá específicamente cómo funciona el modelo, qué funcionalidades son más importantes, o cómo cambiar el resultado**. Esto genera una **asimetría informativa radical**: la plataforma sabe exactamente cómo manipulará el comportamiento del usuario; el usuario no sabe nada.

**El Tribunal Supremo español ha reconocido recientemente la importancia fundamental de esta asimetría informativa. En la Sentencia n.º 3826/2025, de 11 de septiembre, dictada en el recurso de casación sobre la aplicación informática BOSCO[\[34\]](#footnote-35) (utilizada para determinar la elegibilidad para el bono social), el Tribunal estableció un principio constitucional crítico: que existe un derecho ciudadano al acceso a información sobre algoritmos cuando estos afectan derechos sociales, fundamentado en exigencias de democracia y transparencia derivadas del artículo 105.b de la Constitución Española[\[35\]](#footnote-36). Al fundamentar esta decisión, el Tribunal reconoció la complejidad inherente a la ponderación entre transparencia algorítmica y seguridad de la información. Subrayó que se debe prestar "especial atención a los novedosos y complejos matices que la problemática suscitada en este recurso plantea, relativos a la actividad automatizada de la Administración a través de aplicaciones informáticas"[\[36\]](#footnote-37). Sin embargo, al conceder el acceso completo al código fuente a pesar de los argumentos de seguridad cibernética esgrimidos por la Administración, el Tribunal declaró implícitamente que el derecho a la transparencia algorítmica sobre sistemas que afectan derechos sociales fundamentales prevalece sobre la invocación de riesgos de ciberseguridad como justificación para mantener la opacidad.**

**Es menester recalcar que aunque esta sentencia trata sobre algoritmos públicos utilizados en la Administración, su lógica es directamente aplicable al contexto privado: si el Tribunal Supremo considera que el derecho a la transparencia algorítmica prevalece sobre argumentos de seguridad cibernética en el sector público, donde existen mecanismos de control administrativo y judicial, con mayor razón debe prevalecer sobre argumentos de secreto comercial o propiedad intelectual en el sector privado, donde carecen de tales controles democráticos. Las plataformas digitales privadas no pueden ampararse en protecciones que ni siquiera la Administración pudo invocar exitosamente para eludir la transparencia sobre sistemas que determinan qué información ven los ciudadanos y cómo son perfilados. En consecuencia, sin información accesible y verificable sobre cómo funcionan los algoritmos de perfilado y recomendación, el consentimiento del usuario no puede considerarse verdaderamente "informado" en el sentido del RGPD, y la ficción legal del consentimiento se perpetúa.**

**A lo largo del presente ensayo se han identificado y analizado algunos problemas jurídicos fundamentales que demuestran cómo el perfilado algorítmico combinado con manipulación conductual constituye una violación estructural de derechos fundamentales en el ordenamiento jurídico europeo.** La conclusión jurídica es clara: se requiere reforma legislativa en tres niveles simultáneamente. A nivel europeo, la Ley de IA debe implementarse de forma robusta para prohibir efectivamente sistemas que utilicen manipulación subliminal. A nivel español, la AEPD debe intensificar sanciones con efecto disuasorio real, los tribunales deben consolidar el derecho de acceso a algoritmos propietarios siguiendo la jurisprudencia de transparencia algorítmica del caso Civio. A nivel de empresa, los principios de Data Protection by Design y accountability deben traducirse en auditorías obligatorias y responsabilidad personal de directivos por culturas que normalizan la manipulación conductual.

**Sin embargo, más allá de estos imperativos de reforma legislativa, existe una reflexión fundamental que debe motivar la acción: ¿qué significa realmente "privacidad" en la era del perfilado algorítmico? Históricamente entendida como protección de información íntima, la privacidad debe reconceptualizarse como derecho a no ser predefinido por perfiles que cierren futuros posibles. La privacidad comportamental es el derecho a que mi futuro no sea completamente predecible y manipulado, a ser algo distinto de lo que los algoritmos predijeron, a tomar decisiones por mí mismo. Por esta razón, la lucha contra perfilado algorítmico y manipulación conductual es una lucha por democracia, dignidad humana y paz social, no meramente por cumplimiento regulatorio. Es una lucha por garantizar que la arquitectura digital sea diseñada para maximizar libertad y autonomía, no control y manipulación.**

**Bibliografía**

-   Agencia Española de Protección de Datos, ‘Resolución Del Procedimiento Sancionador PS/00300/2019 Contra Vueling Airlines, S.A.’ <https://www.aepd.es/documento/ps-00300-2019.pdf>
-   ‘Guía de Protección de Datos Por Defecto’ (Agencia Española de Protección de Datos 2020) Guía <[https://www.aepd.es/guias/guia-proteccion-datos-por-defecto.pdf](https://www.aepd.es/guias/guia-proteccion-datos-por-defecto.pdf)\>
-   ‘Dark patterns: Manipulación en los servicios de Internet | AEPD’ (*AEDP*, 19 May 2022) <https://www.aepd.es/prensa-y-comunicacion/blog/dark-patterns-manipulacion-en-los-servicios-de-internet> accessed 7 December 2025
-   ‘Patrones Adictivos En El Tratamiento de Datos Personales: Implicaciones Para La Protección de Datos’ (Agencia Española de Protección de Datos 2024) Informe <[https://www.aepd.es/guias/patrones-adictivos-en-tratamiento-de-datos-personales.pdf](https://www.aepd.es/guias/patrones-adictivos-en-tratamiento-de-datos-personales.pdf)\>
-   Amnistía Internacional, ‘Gigantes de La Vigilancia: La Amenaza Que El Modelo de Negocios de Google y Facebook Representa Para Los Derechos Humanos’ (Amnesty International Ltd 2019) Informe POL 30/1404/2019
-   Autoriteit Consument & Markt, ‘Google Must Better Comply with Consumer Rights | ACM’ (23 July 2021) <https://www.acm.nl/en/publications/google-must-better-comply-consumer-rights> accessed 8 December 2025
-   Directorate-General for Justice and Consumers (European Commission), *Behavioural Study on Unfair Commercial Practices in the Digital Environment: Dark Patterns and Manipulative Personalisation : Final Report* (Publications Office of the European Union 2022) <https://data.europa.eu/doi/10.2838/859030> accessed 7 December 2025
-   European Data Protection Board, ‘Guidelines 05/2020 on Consent under Regulation 2016/679’ (European Data Protection Board 2020) Guidelines Guidelines 05/2020 <[https://www.edpb.europa.eu/sites/default/files/files/file1/edpb\_guidelines\_202005\_consent\_en.pdf](https://www.edpb.europa.eu/sites/default/files/files/file1/edpb_guidelines_202005_consent_en.pdf)\>
-   ‘Guidelines 3/2022 on Deceptive Design Patterns in Social Media Platform Interfaces: How to Recognise and Avoid Them’ (European Data Protection Board 2023) Guidelines Guidelines 3/2022 <[https://www.edpb.europa.eu/system/files/2023-02/edpb\_03-2022\_guidelines\_on\_deceptive\_design\_patterns\_in\_social\_media\_platform\_interfaces\_v2\_en\_0.pdf](https://www.edpb.europa.eu/system/files/2023-02/edpb_03-2022_guidelines_on_deceptive_design_patterns_in_social_media_platform_interfaces_v2_en_0.pdf)\>
-   ‘EDPB: “Consent or Pay” Models Should Offer Real Choice | European Data Protection Board’ <https://www.edpb.europa.eu/news/news/2024/edpb-consent-or-pay-models-should-offer-real-choice\_en> accessed 8 December 2025
-   European Parliament. Directorate General for Parliamentary Research Services., *The Impact of the General Data Protection Regulation on Artificial Intelligence.* (Publications Office 2020) <https://data.europa.eu/doi/10.2861/293> accessed 7 December 2025
-   ‘REGLAMENTO (UE) 2016/ 679 DEL PARLAMENTO EUROPEO Y DEL CONSEJO - de 27 de abril de 2016 - relativo a la protección de las personas físicas en lo que respecta al tratamiento de datos personales y a la libre circulación de estos datos y por el que se deroga la Directiva 95/ 46/ CE (Reglamento general de protección de datos)’
-   The European Consumer Organisation (BEUC), ‘BEUC Files Complaint against TikTok for Multiple EU Consumer Law Breaches’ <[https://www.beuc.eu/sites/default/files/publications/beuc-pr-2021-006\_beuc\_files\_complaint\_against\_tiktok\_for\_multiple\_eu\_consumer\_law\_breaches.pdf](https://www.beuc.eu/sites/default/files/publications/beuc-pr-2021-006_beuc_files_complaint_against_tiktok_for_multiple_eu_consumer_law_breaches.pdf)
-   Tribunal Supremo, ‘Sentencia del Tribunal Supremo 1119/2025 (Caso Civio - Código Fuente BOSCO)’
-   Wachter S and Mittelstadt B, ‘A Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI’ <https://osf.io/mu2kf\_v1> accessed 8 December 2025
-   Reglamento (UE) 2024/1689 del Parlamento Europeo y del Consejo, de 13 de junio de 2024, por el que se establecen normas armonizadas en materia de inteligencia artificial y por el que se modifican los Reglamentos (CE) no 300/2008, (UE) no 167/2013, (UE) no 168/2013, (UE) 2018/858, (UE) 2018/1139 y (UE) 2019/2144 y las Directivas 2014/90/UE, (UE) 2016/797 y (UE) 2020/1828 (Reglamento de Inteligencia Artificial) 2024 (Reglamento (UE) 2024/1689) 144

1.  Agencia Española de Protección de Datos, ‘Dark patterns: Manipulación en los servicios de Internet | AEPD’ (*AEDP*, 19 May 2022) <https://www.aepd.es/prensa-y-comunicacion/blog/dark-patterns-manipulacion-en-los-servicios-de-internet> accessed 7 December 2025. [↑](#footnote-ref-2)
    
2.  Directorate-General for Justice and Consumers (European Commission), *Behavioural Study on Unfair Commercial Practices in the Digital Environment: Dark Patterns and Manipulative Personalisation : Final Report* (Publications Office of the European Union 2022) <https://data.europa.eu/doi/10.2838/859030> accessed 7 December 2025. [↑](#footnote-ref-3)
    
3.  ‘REGLAMENTO (UE) 2016/ 679 DEL PARLAMENTO EUROPEO Y DEL CONSEJO - de 27 de abril de 2016 - relativo a la protección de las personas físicas en lo que respecta al tratamiento de datos personales y a la libre circulación de estos datos y por el que se deroga la Directiva 95/ 46/ CE (Reglamento general de protección de datos)’ art 5.1.a. [↑](#footnote-ref-4)
    
4.  ibid 22. [↑](#footnote-ref-5)
    
5.  Reglamento (UE) 2024/1689 del Parlamento Europeo y del Consejo, de 13 de junio de 2024, por el que se establecen normas armonizadas en materia de inteligencia artificial y por el que se modifican los Reglamentos (CE) no 300/2008, (UE) no 167/2013, (UE) no 168/2013, (UE) 2018/858, (UE) 2018/1139 y (UE) 2019/2144 y las Directivas 2014/90/UE, (UE) 2016/797 y (UE) 2020/1828 (Reglamento de Inteligencia Artificial) 2024 (Reglamento (UE) 2024/1689) 144, art 5.1.a. [↑](#footnote-ref-6)
    
6.  Amnistía Internacional, ‘Gigantes de La Vigilancia: La Amenaza Que El Modelo de Negocios de Google y Facebook Representa Para Los Derechos Humanos’ (Amnesty International Ltd 2019) Informe POL 30/1404/2019. [↑](#footnote-ref-7)
    
7.  ‘REGLAMENTO (UE) 2016/ 679 DEL PARLAMENTO EUROPEO Y DEL CONSEJO - de 27 de abril de 2016 - relativo a la protección de las personas físicas en lo que respecta al tratamiento de datos personales y a la libre circulación de estos datos y por el que se deroga la Directiva 95/ 46/ CE (Reglamento general de protección de datos)’ (n 4) art 5.1.a. [↑](#footnote-ref-8)
    
8.  Agencia Española de Protección de Datos, ‘Guía de Protección de Datos Por Defecto’ (Agencia Española de Protección de Datos 2020) Guía 20 <https://www.aepd.es/guias/guia-proteccion-datos-por-defecto.pdf>. [↑](#footnote-ref-9)
    
9.  European Data Protection Board, ‘Guidelines 3/2022 on Deceptive Design Patterns in Social Media Platform Interfaces: How to Recognise and Avoid Them’ (European Data Protection Board 2023) Guidelines Guidelines 3/2022 17 <https://www.edpb.europa.eu/system/files/2023-02/edpb\_03-2022\_guidelines\_on\_deceptive\_design\_patterns\_in\_social\_media\_platform\_interfaces\_v2\_en\_0.pdf>. [↑](#footnote-ref-10)
    
10.  ibid 22. [↑](#footnote-ref-11)
     
11.  ibid. [↑](#footnote-ref-12)
     
12.  ibid 21. [↑](#footnote-ref-13)
     
13.  ibid 45. [↑](#footnote-ref-14)
     
14.  Agencia Española de Protección de Datos, ‘Patrones Adictivos En El Tratamiento de Datos Personales: Implicaciones Para La Protección de Datos’ (Agencia Española de Protección de Datos 2024) Informe 5 <https://www.aepd.es/guias/patrones-adictivos-en-tratamiento-de-datos-personales.pdf>. [↑](#footnote-ref-15)
     
15.  ibid 16. [↑](#footnote-ref-16)
     
16.  ibid 20. [↑](#footnote-ref-17)
     
17.  ibid 27. [↑](#footnote-ref-18)
     
18.  ibid 28. [↑](#footnote-ref-19)
     
19.  ibid 7. [↑](#footnote-ref-20)
     
20.  Reglamento de Inteligencia Artificial 5.1.a. [↑](#footnote-ref-21)
     
21.  Agencia Española de Protección de Datos, ‘Resolución Del Procedimiento Sancionador PS/00300/2019 Contra Vueling Airlines, S.A.’ <https://www.aepd.es/documento/ps-00300-2019.pdf>. [↑](#footnote-ref-22)
     
22.  ‘REGLAMENTO (UE) 2016/ 679 DEL PARLAMENTO EUROPEO Y DEL CONSEJO - de 27 de abril de 2016 - relativo a la protección de las personas físicas en lo que respecta al tratamiento de datos personales y a la libre circulación de estos datos y por el que se deroga la Directiva 95/ 46/ CE (Reglamento general de protección de datos)’ (n 4) art 4.11. [↑](#footnote-ref-23)
     
23.  European Data Protection Board, ‘Guidelines 05/2020 on Consent under Regulation 2016/679’ (European Data Protection Board 2020) Guidelines Guidelines 05/2020 7 <https://www.edpb.europa.eu/sites/default/files/files/file1/edpb\_guidelines\_202005\_consent\_en.pdf>. [↑](#footnote-ref-24)
     
24.  ibid. [↑](#footnote-ref-25)
     
25.  ibid. [↑](#footnote-ref-26)
     
26.  European Data Protection Board, ‘EDPB: “Consent or Pay” Models Should Offer Real Choice | European Data Protection Board’ <https://www.edpb.europa.eu/news/news/2024/edpb-consent-or-pay-models-should-offer-real-choice\_en> accessed 8 December 2025. [↑](#footnote-ref-27)
     
27.  Autoriteit Consument & Markt, ‘Google Must Better Comply with Consumer Rights | ACM’ (23 July 2021) <https://www.acm.nl/en/publications/google-must-better-comply-consumer-rights> accessed 8 December 2025. [↑](#footnote-ref-28)
     
28.  Amnistía Internacional (n 7) 8. [↑](#footnote-ref-29)
     
29.  European Data Protection Board, ‘Guidelines 05/2020 on Consent under Regulation 2016/679’ (n 25) para 39. [↑](#footnote-ref-30)
     
30.  ibid 41. [↑](#footnote-ref-31)
     
31.  The European Consumer Organisation (BEUC), ‘BEUC Files Complaint against TikTok for Multiple EU Consumer Law Breaches’ <https://www.beuc.eu/sites/default/files/publications/beuc-pr-2021-006\_beuc\_files\_complaint\_against\_tiktok\_for\_multiple\_eu\_consumer\_law\_breaches.pdf>. [↑](#footnote-ref-32)
     
32.  Sandra Wachter and Brent Mittelstadt, ‘A Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI’ 614 <https://osf.io/mu2kf\_v1> accessed 8 December 2025. [↑](#footnote-ref-33)
     
33.  European Data Protection Board, ‘Guidelines 05/2020 on Consent under Regulation 2016/679’ (n 25) para 62. [↑](#footnote-ref-34)
     
34.  Tribunal Supremo, ‘Sentencia del Tribunal Supremo 1119/2025 (Caso Civio - Código Fuente BOSCO)’. [↑](#footnote-ref-35)
     
35.  ibid 22. [↑](#footnote-ref-36)
     
36.  ibid 23. [↑](#footnote-ref-37)