**Data Protection and Privacy Violations**

**through Algorithmic Profiling and Behavioral Manipulation:**

**the challenges facing European and Spanish regulation.**

In the contemporary digital era, privacy is not violated solely through the massive collection of personal data. There exists a more insidious and less visible dimension of fundamental rights violations: that which occurs through deliberate manipulation of user decisions via interface design that, while apparently functional and neutral, is specifically engineered to influence behavior in potentially harmful ways. The so-called "dark patterns" represent an emergent category of rights violations that European legislation is only beginning to address systematically.

According to the Spanish Data Protection Authority (AEPD), dark patterns are "user interfaces and user experience implementations designed to influence the behavior and decisions of people in their interaction with websites, apps, and social networks, such that they make decisions potentially harmful to the protection of their personal data"[1]. The magnitude of the problem is alarming: recent interdisciplinary academic studies reveal that these practices are systemic, not exceptional, penetrating the vast majority of the most popular digital services[2].

In this regard, one must ask: How can true privacy and data protection exist if users are constantly manipulated through interfaces deliberately designed to undermine their decisional autonomy? And adjacently: What counterbalances has European and Spanish legislation established to ensure the protection of user rights against this manipulation? These questions motivate the present essay.

The violation of privacy through algorithmic profiling and behavioral manipulation exists at the intersection of three European regulatory frameworks. The GDPR (articles 5.1.a and 22) establishes the principle of "lawfulness, fairness, and transparency" in personal data processing[3] and recognizes a right not to be subject to automated decisions producing significant legal effects[4]. However, this framework traditionally focuses on data protection rather than protection against behavioral manipulation. The Artificial Intelligence Act (articles 5.1.a and 5.1.b) explicitly prohibits systems employing manipulative techniques[5], thereby recognizing that the GDPR alone is insufficient to protect against these emergent practices. Finally, the AEPD has begun to specifically sanction the use of dark patterns as violations of the fairness principle of the GDPR, establishing administrative case law on this matter that will be addressed in the next section.

**Dark Patterns, Addictive Patterns, and Digital Manipulation**

The violation of data protection through algorithmic profiling and behavioral manipulation is not an exceptional phenomenon. It is a specific and systematic category of violations operating at the intersection between deliberately designed user interfaces aimed at manipulating decisions and systems exploiting human psychological vulnerabilities[6].

As noted above, dark patterns are interfaces designed to influence and negatively affect the behavior and decisions of users. Legally, these patterns violate article 5.1.a) of the GDPR, which establishes that personal data shall be processed in a manner "lawful, fair, and transparent in relation to the data subject"[7]. In this sense, the fairness principle is not merely a generic information obligation; it is a substantive obligation not to deliberately manipulate data subjects in their decisions regarding data. The AEPD has explicitly specified that "in application of the fairness principle established in article 5.1.a, data controllers must ensure that dark patterns are not employed, at minimum, in relation to decisions concerning the processing of their personal data"[8].

The European Data Protection Board (EDPB) has developed a classification of dark patterns that the AEPD has adopted. The five main types are: Overload[9], which generates cognitive fatigue through multiple simultaneous options; Obstruction[10], which hides important information; Emotion[11], which appeals to emotions to influence decisions; Impediment[12], which creates technical difficulties for exercising rights; and Inconsistency[13], which presents unstable design where controls change without notice.

Meanwhile, while dark patterns attack freedom of decision through user interface design, there exists a broader category of manipulative practices: addictive patterns. The AEPD defines these as "a feature, attribute, or design practice that determines a specific manner of using digital platforms, applications, or services that seeks to have users dedicate much more time to using them or with a greater degree of engagement than expected, convenient, or healthy for them"[14]. Addictive patterns function by exploiting specific cognitive biases and psychological vulnerabilities of the user. The AEPD classifies these patterns into four main categories: Forced Action[15], which offers something desirable in exchange for demands or deception (autoplay, infinite scroll, push notifications); Social Engineering[16], which manipulates through cognitive biases (artificial scarcity, social proof, false countdown timers); Interface Interference[17], which manipulates visual elements to promote actions; and Persistence[18], which exploits psychological effects such as the Zeigarnik effect.

What is significant is that the AEPD has recognized that when these addictive patterns are related to "subliminal, manipulative, deceptive techniques or those exploiting vulnerabilities of natural persons or groups of persons"[19], especially by age, such that they are mediated by automated AI systems, we would be confronted with behaviors prohibited under article 5 of the European Artificial Intelligence Regulation. This recognition points toward the most serious category of manipulation: the subliminal techniques prohibited by the AI Act. In this sense, article 5.1.a) of the AI Act establishes that "the marketing, placing on the market, or use of an AI system that deploys subliminal techniques that escape the consciousness of a person or deliberately manipulative or deceptive techniques, with the objective or effect of materially distorting the behavior of a person" are prohibited[20]. This is an absolute prohibition, not merely a regulatory norm with flexible sanctions; there is no partial compliance: the system cannot be used under any circumstance pursuant to article 5.1.a).

Dark patterns, addictive patterns, and subliminal techniques constitute violations operating at multiple levels of the European legal order. At the GDPR level, they violate fairness as a substantive obligation not to deliberately manipulate. At the AI Act level, they constitute expressly prohibited systems. And at the practical level, the AEPD has already begun to sanction these practices as concrete GDPR violations[21]. Algorithmic profiling combined with these manipulation mechanisms does not represent a marginal problem but a specific category of fundamental rights violation affecting autonomy, privacy, and dignity.

**The Problem with Algorithmic Profiling and Informed Consent (Under Pressure)**

There exists a central paradox of consent in the context of algorithmic profiling: while the GDPR requires that it be **"free, specific, informed, and unambiguous,"** in practice consent to profiling is obtained under pressure to access services (we cannot use digital platforms without consenting to profiling) and through dark patterns (interfaces designed to manipulate decisions), as noted earlier. This combination renders consent practically a legal fiction: valid on paper, but substantively null in practice.

The GDPR defines consent in its article 4.11 as **"any manifestation of will that is freely given, specific, informed, and unambiguous by which the data subject, by a statement or by a clear affirmative action, accepts the processing of personal data concerning him or her"[22]**. In this sense, and according to its essential elements, consent must be: **Free, in the sense that** the person must not have been pressured to give consent nor suffer detriment if rejecting it; **Specific, implying that** the person must be asked to consent to individual specific types of data processing; **Informed, in that** the person must be informed of what they are consenting to; **Unambiguous, because** the language must be clear and simple; and must constitute a clear **Affirmative Action, meaning that** the person must expressly consent by doing or saying something, not through silence or inactivity[23].

The European Data Protection Board (EDPB) has established in its **Guidelines 5/2020 on Consent** that **"consent must be free, which implies real choice and control by data subjects[24]**. It is understood that if the subject is not truly free to choose, feels obligated to give consent, or will suffer negative consequences if they do not, then consent cannot be considered valid. The EDPB explicitly specifies that **"if consent is included as a non-negotiable part of general terms and conditions, it is presumed that it has not been freely given[25]**. Consequently, it can be stated that consent cannot be considered freely given if the data subject cannot refuse or withdraw consent without detriment.

This guideline is critical in the context of algorithmic profiling. Users of social networks, search engines, and streaming platforms cannot access these services without consenting to exhaustive profiling for behavioral advertising[26]. There exists no option to use YouTube without accepting that Google collects data on our preferences for personalization of recommendations[27]. There exists no option to use Instagram without accepting that Meta constructs a psychographic profile for targeted advertising[28]. In this sense, consent is **a non-negotiable condition of access**, and therefore not a genuine option for users.

**The Magnitude of the Legal Problem: Dark Patterns + Consent = Nullity**

When dark patterns are combined with pressure to access, the nullity of consent is amplified. The EDPB provides a specifically relevant example for digital platforms: **"A website provider introduces a script that hides content except for a request to accept cookies and information about the cookies used and the purposes for which data will be processed. It is not possible to access the content without clicking the 'accept cookies' button. Given that the data subject is not offered a real possibility of choice, their consent is not manifested freely"[29]**. The EDPB concludes: **"This does not constitute valid consent, since the provision of the service is conditioned on the data subject clicking the 'accept cookies' button. They are not offered a real possibility of choice"[30]**.

This analysis extends directly to algorithmic profiling on social networks. A TikTok user wishing to use the platform faces an interface where refusing profiling requires multiple clicks, navigation through deep menus, or is simply impossible, as consent is the only functional option (dark pattern of **impediment**)[31]. The pressure to access (I cannot use TikTok without consenting to profiling) combined with dark patterns (refusing is practically impossible) generates a **legal presumption that consent has not been freely given**.

**Algorithmic Opacity as a Barrier to "Informed" Consent**

Beyond problems of "free" and "specific," there exists a third problem: consent **cannot be "informed"** when the logic of the algorithm is opaque[32].

For consent to be "informed," pursuant to article 7.3 of the GDPR, the user must understand: (1) **what data is collected** (name, location, search history, interaction patterns); (2) **how it will be used** (what machine learning model will process it, what features will be extracted, what predictions will be made); (3) **what decisions will result** (what content is recommended, what is hidden, what advertisements are shown); and (4) **who will have access** (advertisers, data brokers, governments)[33].

Disturbingly, platforms frequently invoke **trade secrets and proprietary artificial intelligence** to deny specific information about how their recommendation systems function. A person reading the privacy policy of a modern social network will find generic statements such as "we use machine learning to personalize content," but **will never specifically know how the model works, which features are most important, or how to change the result**. This generates a **radical informational asymmetry**: the platform knows exactly how it will manipulate the user's behavior; the user knows nothing.

**The Spanish Supreme Court has recently recognized the fundamental importance of this informational asymmetry. In Judgment n.º 3826/2025, of September 11, handed down in the cassation appeal regarding the algorithmic application BOSCO[34] (used to determine eligibility for the social bonus), the Court established a critical constitutional principle: that there exists a citizen's right of access to information about algorithms when these affect social rights, grounded in requirements of democracy and transparency derived from article 105.b of the Spanish Constitution[35]. In grounding this decision, the Court recognized the inherent complexity in balancing algorithmic transparency against information security. It emphasized that "special attention must be paid to the novel and complex nuances that the issues raised in this appeal present, concerning automated activity of the Administration through software applications"[36]. However, in granting full access to the source code despite cybersecurity arguments advanced by the Administration, the Court implicitly declared that the right to algorithmic transparency regarding systems affecting fundamental social rights prevails over the invocation of cybersecurity risks as justification for maintaining opacity.**

**It is necessary to emphasize that although this judgment concerns algorithms used by the Public Administration, its logic is directly applicable to the private context: if the Supreme Court considers that the right to algorithmic transparency prevails over cybersecurity arguments in the public sector, where administrative and judicial control mechanisms exist, all the more so must it prevail over invocations of trade secrets or intellectual property in the private sector, where such democratic controls are lacking. Private digital platforms cannot shelter under protections that even the Administration could not successfully invoke to evade transparency regarding systems that determine what information citizens see and how they are profiled. Consequently, without accessible and verifiable information about how profiling and recommendation algorithms function, user consent cannot be considered truly "informed" in the sense of the GDPR, and the legal fiction of consent perpetuates itself.**

**Throughout the present essay, fundamental legal problems have been identified and analyzed that demonstrate how algorithmic profiling combined with behavioral manipulation constitutes a structural violation of fundamental rights in the European legal order.** The legal conclusion is clear: legislative reform at three levels simultaneously is required. At the European level, the AI Act must be robustly implemented to effectively prohibit systems employing subliminal manipulation. At the Spanish level, the AEPD must intensify sanctions with genuine deterrent effect; courts must consolidate the right of access to proprietary algorithms following the algorithmic transparency jurisprudence of the Civio case. At the corporate level, the principles of Data Protection by Design and accountability must translate into mandatory audits and personal responsibility of executives for cultures that normalize behavioral manipulation.

**However, beyond these imperatives of legislative reform, there exists a fundamental reflection that must motivate action: what does "privacy" truly mean in the age of algorithmic profiling? Historically understood as protection of intimate information, privacy must be reconceptualized as the right not to be predefined by profiles that foreclose possible futures. Behavioral privacy is the right that my future not be entirely predictable and manipulated, to be something distinct from what algorithms predicted, to make decisions for myself. For this reason, the struggle against algorithmic profiling and behavioral manipulation is a struggle for democracy, human dignity, and social peace, not merely for regulatory compliance. It is a struggle to ensure that digital architecture is designed to maximize freedom and autonomy, not control and manipulation.**

**Bibliography**

- Spanish Data Protection Authority, ‘Resolution of Sanctioning Procedure PS/00300/2019 Against Vueling Airlines, S.A.’ <https://www.aepd.es/documento/ps-00300-2019.pdf>
- ‘Data Protection by Design Guide’ (Spanish Data Protection Authority 2020) Guide <https://www.aepd.es/guias/guia-proteccion-datos-por-defecto.pdf>
- ‘Dark patterns: Manipulation in Internet Services | AEPD’ (*AEPD*, 19 May 2022) <https://www.aepd.es/prensa-y-comunicacion/blog/dark-patterns-manipulacion-en-los-servicios-de-internet> accessed 7 December 2025
- ‘Addictive Patterns in Personal Data Processing: Implications for Data Protection’ (Spanish Data Protection Authority 2024) Report <https://www.aepd.es/guias/patrones-adictivos-en-tratamiento-de-datos-personales.pdf>
- Amnesty International, ‘Surveillance Giants: The Threat That the Business Model of Google and Facebook Represents for Human Rights’ (Amnesty International Ltd 2019) Report POL 30/1404/2019
- Consumer and Market Authority, ‘Google Must Better Comply with Consumer Rights | ACM’ (23 July 2021) <https://www.acm.nl/en/publications/google-must-better-comply-consumer-rights> accessed 8 December 2025
- Directorate-General for Justice and Consumers (European Commission), *Behavioural Study on Unfair Commercial Practices in the Digital Environment: Dark Patterns and Manipulative Personalisation : Final Report* (Publications Office of the European Union 2022) <https://data.europa.eu/doi/10.2838/859030> accessed 7 December 2025
- European Data Protection Board, ‘Guidelines 05/2020 on Consent under Regulation 2016/679’ (European Data Protection Board 2020) Guidelines <https://www.edpb.europa.eu/sites/default/files/files/file1/edpb_guidelines_202005_consent_en.pdf>
- ‘Guidelines 3/2022 on Deceptive Design Patterns in Social Media Platform Interfaces: How to Recognise and Avoid Them’ (European Data Protection Board 2023) Guidelines <https://www.edpb.europa.eu/system/files/2023-02/edpb_03-2022_guidelines_on_deceptive_design_patterns_in_social_media_platform_interfaces_v2_en_0.pdf>
- ‘EDPB: “Consent or Pay” Models Should Offer Real Choice | European Data Protection Board’ <https://www.edpb.europa.eu/news/news/2024/edpb-consent-or-pay-models-should-offer-real-choice_en> accessed 8 December 2025
- European Parliament. Directorate General for Parliamentary Research Services., *The Impact of the General Data Protection Regulation on Artificial Intelligence.* (Publications Office 2020) <https://data.europa.eu/doi/10.2861/293> accessed 7 December 2025
- ‘REGULATION (EU) 2016/679 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL - of 27 April 2016 - on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)’
- The European Consumer Organisation (BEUC), ‘BEUC Files Complaint against TikTok for Multiple EU Consumer Law Breaches’ <https://www.beuc.eu/sites/default/files/publications/beuc-pr-2021-006_beuc_files_complaint_against_tiktok_for_multiple_eu_consumer_law_breaches.pdf>
- Spanish Supreme Court, ‘Judgment of the Spanish Supreme Court 1119/2025 (Civio Case - BOSCO Source Code)’
- Wachter S and Mittelstadt B, ‘A Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI’ <https://osf.io/mu2kf_v1> accessed 8 December 2025
- Regulation (EU) 2024/1689 of the European Parliament and of the Council, of 13 June 2024, establishing harmonised rules on artificial intelligence and amending Regulations (EC) no 300/2008, (EU) no 167/2013, (EU) no 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Regulation) 2024 (Regulation (EU) 2024/1689)
